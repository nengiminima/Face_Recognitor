{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.62.3)\n",
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "     |████████████████████████████████| 108 kB 17.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow_hub) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow_hub) (3.15.2)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n",
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "     |████████████████████████████████| 516.2 MB 10 kB/s               \n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "     |████████████████████████████████| 2.9 MB 45.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.15.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.44.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     |████████████████████████████████| 4.3 MB 90.5 MB/s            \n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 81.8 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "     |████████████████████████████████| 454 kB 78.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.19.5)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "     |████████████████████████████████| 26.1 MB 51.1 MB/s            \n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 1.5 MB/s             \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 6.1 MB/s              \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.36.2)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "     |████████████████████████████████| 3.0 MB 53.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.2)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 104.8 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 106.7 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 11.4 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 107.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.5.30)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 112.3 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=4ae08d03ba7a47acef1b2850ba3d84330d857627397f37b5e1238a06cd452fb8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, scipy, opt-einsum, keras-preprocessing, h5py, gast, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.7.0\n",
      "    Uninstalling importlib-metadata-3.7.0:\n",
      "      Successfully uninstalled importlib-metadata-3.7.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.3\n",
      "    Uninstalling scipy-1.5.3:\n",
      "      Successfully uninstalled scipy-1.5.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 google-auth-oauthlib-0.4.6 grpcio-1.44.0 h5py-2.10.0 importlib-metadata-4.8.3 keras-preprocessing-1.1.2 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 scipy-1.4.1 tensorboard-2.2.2 tensorboard-plugin-wit-1.8.1 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install tensorflow_hub\n",
    "!pip install tensorflow==2.2.0\n",
    "import os\n",
    "import s3fs\n",
    "import math\n",
    "import tqdm\n",
    "import boto3\n",
    "import random\n",
    "#import mxnet as mx\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import (MobileNetV2,ResNet50)\n",
    "from tensorflow.keras.layers import (Dense,Dropout,Flatten,Input)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch_size = 128\n",
    "input_size  = 112\n",
    "embd_shape = 512\n",
    "sub_name = 'arc_mobv2' #arc_rec50 or arc_mobv2\n",
    "backbone_type = 'MobileNetV2' # 'ResNet50', 'MobileNetV2'\n",
    "head_type = \"ArcHead\" # 'ArcHead', 'NormHead'\n",
    "is_ccrop = False # central-cropping or not\n",
    "binary_img = True\n",
    "num_classes =  94873 # 9131 # 85743 #\n",
    "num_samples = 8960460 # 3137806 # 5822653 # \n",
    "epochs = 5\n",
    "base_lr = 0.01\n",
    "w_decay = float(5e-4)\n",
    "save_steps = 500\n",
    "tfrecord_path = \"data/ms1m_.tfrecord\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading TFrecords\n",
    "def _parse_tfrecord(binary_img=False, is_ccrop=False):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        if binary_img:\n",
    "            features = {'image/source_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "#                         'image/data_source': tf.io.FixedLenFeature([], tf.string),\n",
    "                        'image/encoded': tf.io.FixedLenFeature([], tf.string)}\n",
    "            x = tf.io.parse_single_example(tfrecord, features)\n",
    "            x_train = tf.image.decode_jpeg(x['image/encoded'], channels=3)\n",
    "        else:\n",
    "            features = {'image/source_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "                        'image/img_path': tf.io.FixedLenFeature([], tf.string)}\n",
    "            x = tf.io.parse_single_example(tfrecord, features)\n",
    "            image_encoded = tf.io.read_file(x['image/img_path'])\n",
    "            x_train = tf.image.decode_jpeg(image_encoded, channels=3)\n",
    "\n",
    "        y_train = tf.cast(x['image/source_id'], tf.float32)\n",
    "        print(y_train)\n",
    "        x_train = _transform_images(is_ccrop=is_ccrop)(x_train)\n",
    "        y_train = _transform_targets(y_train)\n",
    "        return (x_train, y_train), y_train\n",
    "    return parse_tfrecord\n",
    "\n",
    "def _transform_images(is_ccrop=False):\n",
    "    def transform_images(x_train):\n",
    "        x_train = tf.image.resize(x_train, (128, 128))\n",
    "        x_train = tf.image.random_crop(x_train, (input_size, input_size, 3))\n",
    "        x_train = tf.image.random_flip_left_right(x_train)\n",
    "        x_train = tf.image.random_saturation(x_train, 0.6, 1.4)\n",
    "        x_train = tf.image.random_brightness(x_train, 0.4)\n",
    "        x_train = x_train / 255\n",
    "        return x_train\n",
    "    return transform_images\n",
    "\n",
    "def _transform_targets(y_train):\n",
    "    return y_train\n",
    "\n",
    "def load_tfrecord_dataset(tfrecord_name, batch_size, binary_img=False, \n",
    "                          shuffle=True, buffer_size=10240,is_ccrop=False):\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(binary_img=binary_img, is_ccrop=is_ccrop),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backbone(backbone_type='ResNet50', use_pretrain\n",
    "             =True):\n",
    "    \"\"\"Backbone Model\"\"\"\n",
    "    weights = None\n",
    "    if use_pretrain:\n",
    "        weights = 'imagenet'\n",
    "\n",
    "    def backbone(x_in):\n",
    "        if backbone_type == 'ResNet50':\n",
    "            return ResNet50(input_shape=x_in.shape[1:], include_top=False,\n",
    "                            weights=weights)(x_in)\n",
    "        elif backbone_type == 'MobileNetV2':\n",
    "            return MobileNetV2(input_shape=x_in.shape[1:], include_top=False,\n",
    "                               weights=weights)(x_in)\n",
    "        else:\n",
    "            raise TypeError('backbone_type error!')\n",
    "    return backbone\n",
    "\n",
    "\n",
    "def OutputLayer(embd_shape, w_decay=5e-4, name='OutputLayer'):\n",
    "    \"\"\"Output Later\"\"\"\n",
    "    def output_layer(x_in):\n",
    "        x = inputs = Input(x_in.shape[1:])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(embd_shape, kernel_regularizer=tf.keras.regularizers.l2(w_decay) )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        return Model(inputs, x, name=name)(x_in)\n",
    "    return output_layer\n",
    "\n",
    "def ArcHead(num_classes, margin=0.5, logist_scale=64, name='ArcHead'):\n",
    "    \"\"\"Arc Head\"\"\"\n",
    "    def arc_head(x_in, y_in):\n",
    "        x = inputs1 = Input(x_in.shape[1:])\n",
    "        y = Input(y_in.shape[1:])\n",
    "        x = ArcMarginPenaltyLogists(num_classes=num_classes,\n",
    "                                    margin=margin,\n",
    "                                    logist_scale=logist_scale)(x, y)\n",
    "        return Model((inputs1, y), x, name=name)((x_in, y_in))\n",
    "    return arc_head\n",
    "\n",
    "def ArcFaceModel(size, channels, num_classes, name, \n",
    "                 margin, logist_scale, embd_shape, head_type, \n",
    "                 backbone_type, w_decay, use_pretrain, training):\n",
    "    \"\"\"Arc Face Model\"\"\"\n",
    "    x = inputs = Input([size, size, channels], name='input_image')\n",
    "\n",
    "    x = Backbone(backbone_type=backbone_type, use_pretrain=use_pretrain)(x)\n",
    "\n",
    "    embds = OutputLayer(embd_shape, w_decay=w_decay)(x)\n",
    "    if training:\n",
    "        assert num_classes is not None\n",
    "        labels = Input([], name='label')\n",
    "        if head_type == 'ArcHead':\n",
    "            logist = ArcHead(num_classes=num_classes, margin=margin,\n",
    "                             logist_scale=logist_scale)(embds, labels)\n",
    "        else:\n",
    "            logist = NormHead(num_classes=num_classes, w_decay=w_decay)(embds)\n",
    "        return Model((inputs, labels), logist, name=name)\n",
    "    else:\n",
    "        return Model(inputs, embds, name=name)\n",
    "    \n",
    "    #return Model(inputs, embds, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\"Make trainable=False freeze BN for real (the og version is sad).\n",
    "       ref: https://github.com/zzh8829/yolov3-tf2\n",
    "    \"\"\"\n",
    "    def call(self, x, training=False):\n",
    "        if training is None:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "\n",
    "class ArcMarginPenaltyLogists(tf.keras.layers.Layer):\n",
    "    \"\"\"ArcMarginPenaltyLogists\"\"\"\n",
    "    def __init__(self, num_classes, margin=0.5, logist_scale=64, **kwargs):\n",
    "        super(ArcMarginPenaltyLogists, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.logist_scale = logist_scale\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \n",
    "            'num_classes': self.num_classes,\n",
    "            'margin': self.margin,\n",
    "            'logist_scale': self.logist_scale\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_variable(\n",
    "            \"weights\", shape=[int(input_shape[-1]), self.num_classes])\n",
    "        self.cos_m = tf.identity(math.cos(self.margin), name='cos_m')\n",
    "        self.sin_m = tf.identity(math.sin(self.margin), name='sin_m')\n",
    "        self.th = tf.identity(math.cos(math.pi - self.margin), name='th')\n",
    "        self.mm = tf.multiply(self.sin_m, self.margin, name='mm')\n",
    "\n",
    "    def call(self, embds, labels):\n",
    "        normed_embds = tf.nn.l2_normalize(embds, axis=1, name='normed_embd')\n",
    "        normed_w = tf.nn.l2_normalize(self.w, axis=0, name='normed_weights')\n",
    "\n",
    "        cos_t = tf.matmul(normed_embds, normed_w, name='cos_t')\n",
    "        sin_t = tf.sqrt(1. - cos_t ** 2, name='sin_t')\n",
    "\n",
    "        cos_mt = tf.subtract(\n",
    "            cos_t * self.cos_m, sin_t * self.sin_m, name='cos_mt')\n",
    "\n",
    "        cos_mt = tf.where(cos_t > self.th, cos_mt, cos_t - self.mm)\n",
    "\n",
    "        mask = tf.one_hot(tf.cast(labels, tf.int32), depth=self.num_classes,\n",
    "                          name='one_hot_mask')\n",
    "\n",
    "        logists = tf.where(mask == 1., cos_mt, cos_t)\n",
    "        logists = tf.multiply(logists, self.logist_scale, 'arcface_logist')\n",
    "\n",
    "        return logists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftmaxLoss():\n",
    "    \"\"\"softmax loss\"\"\"\n",
    "    def softmax_loss(y_true, y_pred):\n",
    "        # y_true: sparse target\n",
    "        # y_pred: logist\n",
    "        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                            logits=y_pred)\n",
    "        return tf.reduce_mean(ce)\n",
    "    return softmax_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"arcface_model\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)              Output Shape      Param #  Connected to               \n",
      "================================================================================\n",
      "input_image (InputLayer)  [(None, 112, 112, 0                                   \n",
      "________________________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Mod (None, 4, 4, 1280 2257984  input_image[0][0]          \n",
      "________________________________________________________________________________\n",
      "OutputLayer (Model)       (None, 512)       10493440 mobilenetv2_1.00_224[1][0] \n",
      "________________________________________________________________________________\n",
      "label (InputLayer)        [(None,)]         0                                   \n",
      "________________________________________________________________________________\n",
      "ArcHead (Model)           (None, 94873)     48574976 OutputLayer[1][0]          \n",
      "                                                     label[0][0]                \n",
      "================================================================================\n",
      "Total params: 61,326,400\n",
      "Trainable params: 61,288,704\n",
      "Non-trainable params: 37,696\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model\")\n",
    "model = ArcFaceModel(size=input_size,\n",
    "                     channels=3, \n",
    "                     num_classes=num_classes, \n",
    "                     name='arcface_model', \n",
    "                     margin=0.5, \n",
    "                     logist_scale=64, \n",
    "                     embd_shape=embd_shape, \n",
    "                     head_type=head_type, \n",
    "                     backbone_type = backbone_type,\n",
    "                     w_decay=w_decay, \n",
    "                     use_pretrain = True, \n",
    "                     training=True)\n",
    "model.summary(line_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_inf(ckpt_path, steps_per_epoch):\n",
    "    \"\"\"get ckpt information\"\"\"\n",
    "    split_list = ckpt_path.split('e_')[-1].split('_b_')\n",
    "    epochs = int(split_list[0])\n",
    "    batchs = int(split_list[-1].split('.ckpt')[0])\n",
    "    steps = (epochs - 1) * steps_per_epoch + batchs\n",
    "\n",
    "    return epochs, steps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dataset_len = num_samples\n",
    "#path = 9\n",
    "steps_per_epoch = dataset_len // batch_size #==(261)\n",
    "dataset = load_tfrecord_dataset(tfrecord_path, batch_size, binary_img= binary_img,is_ccrop=is_ccrop)\n",
    "learning_rate = tf.constant(base_lr)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "loss_fn = SoftmaxLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] training from scratch.\n"
     ]
    }
   ],
   "source": [
    "print(\"[*] training from scratch.\")\n",
    "epochs, steps = 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   12/70003 [..............................] - ETA: 44:24:54 - loss: 41.4361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-31b6d099a448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     initial_epoch=epochs - 1)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[*] training done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_fn) #Try true\n",
    "check_path = ('./ckpt/ms1m_vgg_ckpt/e_{epoch}_b_{loss}.ckpt')\n",
    "mc_callback = ModelCheckpoint(check_path,save_freq=1000 , verbose=1, save_weights_only=True)\n",
    "tb_callback = TensorBoard(log_dir='logs/', update_freq=batch_size * 5, profile_batch=0)\n",
    "tb_callback._total_batches_seen = steps\n",
    "tb_callback._samples_seen = steps * batch_size\n",
    "callbacks = [mc_callback, tb_callback]\n",
    "\n",
    "history = model.fit(dataset, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    callbacks=callbacks, \n",
    "                    initial_epoch=epochs - 1)\n",
    "\n",
    "print(\"[*] training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./stuf/keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./model', exist_ok=True)\n",
    "model.save('./model/keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newmodel.compile(optimizer=optimizer, loss=loss_fn) #Try true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newmodel.load_weights('./stuf/keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\"Model/SavedModelFormat\")\n",
    "# model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary\n",
    "from tensorflow.python.keras.utils.layer_utils import count_params\n",
    "\n",
    "# trainable_count = count_params(model.trainable_weights)\n",
    "# non_trainable_count = count_params(model.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.Sequential([hub.KerasLayer(\"512_embed\", trainable=True),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "m.build([None, 112, 112, 3])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model_no_quant_tflite = converter.convert()\n",
    "MODEL_NO_QUANT_TFLITE = 'Model/96_embed/model_no_quant.tflite'\n",
    "# # Save the model to disk\n",
    "open(MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model_no_quant_tflite = converter.convert()\n",
    "MODEL_NO_QUANT_TFLITE = 'Model/checkpointss/model_no_quant.tflite'\n",
    "# # Save the model to disk\n",
    "open(MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Open TensorFlow ckpt\n",
    "path = \"ckpt/ms1m_vgg_ckpt/\"\n",
    "reader = tf.train.load_checkpoint(path)\n",
    "\n",
    "print('\\nCount the number of parameters in ckpt file(%s)' % path)\n",
    "param_map = reader.get_variable_to_shape_map()\n",
    "total_count = 0\n",
    "for k, v in param_map.items():\n",
    "    if 'Momentum' not in k and 'global_step' not in k:\n",
    "        temp = np.prod(v)\n",
    "        total_count += temp\n",
    "\n",
    "print('Total Param Count: %d' % total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
